{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23f2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# FILE_PATH = \"../experiments/generated_output.npz\"\n",
    "\n",
    "FILE_PATH = \"../training_dataset/sequences/vowel/S1_NSL_Vowel_Unprepared_Bright/A.npz\"\n",
    "\n",
    "OUTPUT_VIDEO = \"skeleton_check.mp4\"\n",
    "WIDTH, HEIGHT = 800, 800\n",
    "FPS = 60\n",
    "\n",
    "# MediaPipe Connection Maps\n",
    "HAND_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),      # Thumb\n",
    "    (0, 5), (5, 6), (6, 7), (7, 8),      # Index\n",
    "    (5, 9), (9, 10), (10, 11), (11, 12), # Middle\n",
    "    (9, 13), (13, 14), (14, 15), (15, 16), # Ring\n",
    "    (13, 17), (0, 17), (17, 18), (18, 19), (19, 20) # Pinky\n",
    "]\n",
    "\n",
    "POSE_CONNECTIONS = [\n",
    "    (11, 12), (11, 13), (13, 15), # Left arm\n",
    "    (12, 14), (14, 16),           # Right arm\n",
    "    (11, 23), (12, 24), (23, 24)  # Torso\n",
    "]\n",
    "\n",
    "def draw_skeleton(data_path, output_path):\n",
    "    data = np.load(data_path)\n",
    "    pose = data['pose']  \n",
    "    lh = data['lh']      \n",
    "    rh = data['rh']      \n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, FPS, (WIDTH, HEIGHT))\n",
    "\n",
    "    print(f\"Generating video for {len(pose)} frames...\")\n",
    "\n",
    "    for i in range(len(pose)):\n",
    "        frame = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "        # 1. DRAW HANDS\n",
    "        lw_pose = pose[i][15] \n",
    "        rw_pose = pose[i][16]\n",
    "\n",
    "        # In generated data, we don't have visibility, so we check if coordinates are non-zero\n",
    "        if not np.all(lw_pose[:2] == 0):\n",
    "            l_center = (int(lw_pose[0] * WIDTH), int(lw_pose[1] * HEIGHT))\n",
    "        else:\n",
    "            l_center = (200, 400)\n",
    "\n",
    "        if not np.all(rw_pose[:2] == 0):\n",
    "            r_center = (int(rw_pose[0] * WIDTH), int(rw_pose[1] * HEIGHT))\n",
    "        else:\n",
    "            r_center = (600, 400)\n",
    "\n",
    "        centers = {'left': l_center, 'right': r_center}\n",
    "        hand_visual_scale = 200\n",
    "\n",
    "        for side, hand_pts, color in [('left', lh[i], (255, 0, 0)), ('right', rh[i], (0, 0, 255))]:\n",
    "            if np.all(hand_pts == 0): continue\n",
    "            current_center = centers[side]\n",
    "            for start, end in HAND_CONNECTIONS:\n",
    "                p1 = (int(hand_pts[start][0] * hand_visual_scale + current_center[0]), \n",
    "                    int(hand_pts[start][1] * hand_visual_scale + current_center[1]))\n",
    "                p2 = (int(hand_pts[end][0] * hand_visual_scale + current_center[0]), \n",
    "                    int(hand_pts[end][1] * hand_visual_scale + current_center[1]))\n",
    "                cv2.line(frame, p1, p2, color, 2)\n",
    "\n",
    "        # 2. DRAW POSE (Handle 3 or 4 columns)\n",
    "        for start, end in POSE_CONNECTIONS:\n",
    "            p1_raw = pose[i][start]\n",
    "            p2_raw = pose[i][end]\n",
    "            \n",
    "            # --- FIX LOGIC HERE ---\n",
    "            # If the pose has a 4th column, use visibility. \n",
    "            # If it only has 3 columns (Generated), assume visibility = 1.0\n",
    "            p1_vis = p1_raw[3] if len(p1_raw) == 4 else 1.0\n",
    "            p2_vis = p2_raw[3] if len(p2_raw) == 4 else 1.0\n",
    "            \n",
    "            if p1_vis > 0.5 and p2_vis > 0.5:\n",
    "                p1 = (int(p1_raw[0] * WIDTH), int(p1_raw[1] * HEIGHT))\n",
    "                p2 = (int(p2_raw[0] * WIDTH), int(p2_raw[1] * HEIGHT))\n",
    "                cv2.line(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.putText(frame, f\"Frame: {i}\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"âœ… Skeleton video saved to: {output_path}\")\n",
    "\n",
    "# Run the visualization\n",
    "draw_skeleton(FILE_PATH, OUTPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdb0063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def convert_enhanced_npz_to_json(npz_path, output_json_path, video_path_label=None):\n",
    "    \"\"\"\n",
    "    Converts enhanced NPZ files back to JSON.\n",
    "    Reconstructs original coordinates using: (normalized * scale) + wrist\n",
    "    \"\"\"\n",
    "    # 1. Load the NPZ data\n",
    "    data = np.load(npz_path)\n",
    "    \n",
    "    pose_array = data['pose']      # (Frames, 33, 4)\n",
    "    lh_array = data['lh']          # (Frames, 21, 3)\n",
    "    rh_array = data['rh']          # (Frames, 21, 3)\n",
    "    lh_meta = data['lh_meta']      # (Frames, 4) -> [wx, wy, wz, scale]\n",
    "    rh_meta = data['rh_meta']      # (Frames, 4) -> [wx, wy, wz, scale]\n",
    "    \n",
    "    # Extract video info [fps, width, height] saved in NPZ\n",
    "    # If video_info isn't there, we fallback to defaults\n",
    "    if 'video_info' in data:\n",
    "        fps_orig, width, height = data['video_info']\n",
    "    else:\n",
    "        fps_orig, width, height = 60.0, 1920, 1080\n",
    "\n",
    "    # The user specifically requested 'fps': 60 in the JSON\n",
    "    fps_to_use = 60 \n",
    "\n",
    "    # 2. Construct JSON Structure\n",
    "    output_data = {\n",
    "        'metadata': {\n",
    "            'video_path': str(video_path_label) if video_path_label else \"unknown\",\n",
    "            'fps': float(fps_to_use),\n",
    "            'frame_width': int(width),\n",
    "            'frame_height': int(height),\n",
    "            'total_frames': int(pose_array.shape[0]),\n",
    "            'frame_skip': 1,\n",
    "            'hands_swapped': True\n",
    "        },\n",
    "        'frames': []\n",
    "    }\n",
    "\n",
    "    # 3. Process Frames\n",
    "    for i in range(pose_array.shape[0]):\n",
    "        frame_data = {\n",
    "            'frame_number': i + 1,\n",
    "            'timestamp': i / fps_to_use,\n",
    "            'pose': None,\n",
    "            'hands': {'left': None, 'right': None},\n",
    "            'face': None\n",
    "        }\n",
    "\n",
    "        # --- Reconstruct Pose ---\n",
    "        if not np.all(pose_array[i] == 0):\n",
    "            frame_data['pose'] = [\n",
    "                {'x': float(lm[0]), 'y': float(lm[1]), 'z': float(lm[2]), 'visibility': float(lm[3])}\n",
    "                for lm in pose_array[i]\n",
    "            ]\n",
    "\n",
    "        # --- Reconstruct Left Hand ---\n",
    "        # Formula: (normalized_coords * scale) + wrist_position\n",
    "        if not np.all(lh_array[i] == 0):\n",
    "            wx, wy, wz, scale = lh_meta[i]\n",
    "            frame_data['hands']['left'] = [\n",
    "                {\n",
    "                    'x': float((lm[0] * scale) + wx),\n",
    "                    'y': float((lm[1] * scale) + wy),\n",
    "                    'z': float((lm[2] * scale) + wz)\n",
    "                }\n",
    "                for lm in lh_array[i]\n",
    "            ]\n",
    "\n",
    "        # --- Reconstruct Right Hand ---\n",
    "        if not np.all(rh_array[i] == 0):\n",
    "            wx, wy, wz, scale = rh_meta[i]\n",
    "            frame_data['hands']['right'] = [\n",
    "                {\n",
    "                    'x': float((lm[0] * scale) + wx),\n",
    "                    'y': float((lm[1] * scale) + wy),\n",
    "                    'z': float((lm[2] * scale) + wz)\n",
    "                }\n",
    "                for lm in rh_array[i]\n",
    "            ]\n",
    "\n",
    "        output_data['frames'].append(frame_data)\n",
    "\n",
    "    # 4. Save JSON\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Successfully converted {npz_path.name} to {output_json_path}\")\n",
    "\n",
    "# --- Example Usage ---\n",
    "npz_file = Path(\"../experiments/generated_output.npz\")\n",
    "convert_enhanced_npz_to_json(npz_file, \"keypoints.json\", video_path_label=\"S1/A.MOV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbc231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "data = np.load(\"../experiments/generated_output.npz\")\n",
    "print(data['lh'][0])   # First frame left hand\n",
    "print(data['lh'][-1])  # Last frame left hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a8dd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "FILE_PATH = \"../training_dataset/sequences/NSL_Consonant_Multi/S14_NSL_Consonant_RealWorld/S14_NSL_Consonant/BA_883_913.npz\"\n",
    "\n",
    "# FILE_PATH = \"../training_dataset/sequences/consonant/S1_NSL_Consonant_Bright/D_SHA.npz\"\n",
    "\n",
    "OUTPUT_VIDEO = \"skeleton_check.mp4\"\n",
    "WIDTH, HEIGHT = 800, 800\n",
    "FPS = 60\n",
    "\n",
    "# MediaPipe Connection Maps\n",
    "HAND_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),      # Thumb\n",
    "    (0, 5), (5, 6), (6, 7), (7, 8),      # Index\n",
    "    (5, 9), (9, 10), (10, 11), (11, 12), # Middle\n",
    "    (9, 13), (13, 14), (14, 15), (15, 16), # Ring\n",
    "    (13, 17), (0, 17), (17, 18), (18, 19), (19, 20) # Pinky\n",
    "]\n",
    "\n",
    "POSE_CONNECTIONS = [\n",
    "    (11, 12), (11, 13), (13, 15), # Left arm\n",
    "    (12, 14), (14, 16),           # Right arm\n",
    "    (11, 23), (12, 24), (23, 24)  # Torso\n",
    "]\n",
    "\n",
    "def draw_skeleton(data_path, output_path):\n",
    "    data = np.load(data_path)\n",
    "    pose = data['pose']  \n",
    "    lh = data['lh']      \n",
    "    rh = data['rh']      \n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, FPS, (WIDTH, HEIGHT))\n",
    "\n",
    "    print(f\"Generating centered video for {len(pose)} frames...\")\n",
    "\n",
    "    for i in range(len(pose)):\n",
    "        frame = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "        # --- STEP 1: CALCULATE CENTERING OFFSET ---\n",
    "        # We use the shoulders (Landmarks 11 and 12) to find the center of the chest\n",
    "        ls_raw = pose[i][11] # Left Shoulder\n",
    "        rs_raw = pose[i][12] # Right Shoulder\n",
    "        \n",
    "        # Calculate where the chest center is currently (in 0-1 coordinates)\n",
    "        curr_center_x = (ls_raw[0] + rs_raw[0]) / 2\n",
    "        curr_center_y = (ls_raw[1] + rs_raw[1]) / 2\n",
    "        \n",
    "        # Calculate the pixel offset needed to move curr_center to (WIDTH/2, HEIGHT/2)\n",
    "        offset_x = (WIDTH / 2) - (curr_center_x * WIDTH)\n",
    "        offset_y = (HEIGHT / 2) - (curr_center_y * HEIGHT)\n",
    "\n",
    "        # Helper function to apply centering to any point\n",
    "        def get_centered_pt(pt_raw, scale_w, scale_h):\n",
    "            px = int(pt_raw[0] * scale_w + offset_x)\n",
    "            py = int(pt_raw[1] * scale_h + offset_y)\n",
    "            return (px, py)\n",
    "\n",
    "        # --- STEP 2: DRAW POSE (Centered) ---\n",
    "        for start, end in POSE_CONNECTIONS:\n",
    "            p1_raw = pose[i][start]\n",
    "            p2_raw = pose[i][end]\n",
    "            p1_vis = p1_raw[3] if len(p1_raw) == 4 else 1.0\n",
    "            p2_vis = p2_raw[3] if len(p2_raw) == 4 else 1.0\n",
    "            \n",
    "            if p1_vis > 0.5 and p2_vis > 0.5:\n",
    "                p1 = get_centered_pt(p1_raw, WIDTH, HEIGHT)\n",
    "                p2 = get_centered_pt(p2_raw, WIDTH, HEIGHT)\n",
    "                cv2.line(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "        # --- STEP 3: DRAW HANDS (Centered via Wrist) ---\n",
    "        lw_pose = pose[i][15] # Left Wrist Pose Landmark\n",
    "        rw_pose = pose[i][16] # Right Wrist Pose Landmark\n",
    "\n",
    "        # Calculate centered wrist positions to act as anchors for hands\n",
    "        l_anchor = get_centered_pt(lw_pose, WIDTH, HEIGHT)\n",
    "        r_anchor = get_centered_pt(rw_pose, WIDTH, HEIGHT)\n",
    "        \n",
    "        # If pose is missing, use fixed centers\n",
    "        if np.all(lw_pose[:2] == 0): l_anchor = (WIDTH//4, HEIGHT//2)\n",
    "        if np.all(rw_pose[:2] == 0): r_anchor = (3*WIDTH//4, HEIGHT//2)\n",
    "\n",
    "        anchors = {'left': l_anchor, 'right': r_anchor}\n",
    "        hand_visual_scale = 400 # Visual size of hand\n",
    "\n",
    "        for side, hand_pts, color in [('left', lh[i], (255, 0, 0)), ('right', rh[i], (0, 0, 255))]:\n",
    "            if np.all(hand_pts == 0): continue\n",
    "            \n",
    "            # Divide by 5.0 to reverse the training-time scaling\n",
    "            current_hand = hand_pts / 5.0\n",
    "            current_anchor = anchors[side]\n",
    "            \n",
    "            for start, end in HAND_CONNECTIONS:\n",
    "                # Calculate finger positions relative to the centered wrist anchor\n",
    "                p1 = (int(current_hand[start][0] * hand_visual_scale + current_anchor[0]), \n",
    "                      int(current_hand[start][1] * hand_visual_scale + current_anchor[1]))\n",
    "                p2 = (int(current_hand[end][0] * hand_visual_scale + current_anchor[0]), \n",
    "                      int(current_hand[end][1] * hand_visual_scale + current_anchor[1]))\n",
    "                cv2.line(frame, p1, p2, color, 2)\n",
    "\n",
    "            # Draw points\n",
    "            for pt in current_hand:\n",
    "                px = int(pt[0] * hand_visual_scale + current_anchor[0])\n",
    "                py = int(pt[1] * hand_visual_scale + current_anchor[1])\n",
    "                cv2.circle(frame, (px, py), 3, (255, 255, 255), -1)\n",
    "\n",
    "        cv2.putText(frame, f\"Frame: {i}\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"âœ… Centered skeleton video saved to: {output_path}\")\n",
    "    \n",
    "# Run the visualization\n",
    "draw_skeleton(FILE_PATH, OUTPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5519cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "FILE_PATH = \"../experiments/generated_output.npz\"\n",
    "OUTPUT_VIDEO = \"skeleton_check.mp4\"\n",
    "WIDTH, HEIGHT = 1000, 1000 # Larger canvas\n",
    "FPS = 60\n",
    "\n",
    "# MediaPipe Connection Maps\n",
    "HAND_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),      # Thumb\n",
    "    (0, 5), (5, 6), (6, 7), (7, 8),      # Index\n",
    "    (5, 9), (9, 10), (10, 11), (11, 12), # Middle\n",
    "    (9, 13), (13, 14), (14, 15), (15, 16), # Ring\n",
    "    (13, 17), (0, 17), (17, 18), (18, 19), (19, 20) # Pinky\n",
    "]\n",
    "\n",
    "POSE_CONNECTIONS = [\n",
    "    (11, 12), (11, 13), (13, 15), # Left arm\n",
    "    (12, 14), (14, 16),           # Right arm\n",
    "    (11, 23), (12, 24), (23, 24)  # Torso\n",
    "]\n",
    "\n",
    "def draw_generated_skeleton(data_path, output_path):\n",
    "    # Load the npz file\n",
    "    data = np.load(data_path)\n",
    "    pose = data['pose']  # (Frames, 33, 3)\n",
    "    lh = data['lh']      # (Frames, 21, 3)\n",
    "    rh = data['rh']      # (Frames, 21, 3)\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, FPS, (WIDTH, HEIGHT))\n",
    "\n",
    "    # --- VISUAL SCALING PARAMETERS ---\n",
    "    # Since data is normalized by shoulder width (approx 0.3), \n",
    "    # we need a large scale to see it on screen.\n",
    "    VISUAL_SCALE = 500 \n",
    "    OFFSET_X, OFFSET_Y = WIDTH // 2, HEIGHT // 3 # Move skeleton to center-top\n",
    "\n",
    "    print(f\"Generating video for {len(pose)} frames...\")\n",
    "\n",
    "    for i in range(len(pose)):\n",
    "        # Create black canvas\n",
    "        frame = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "        # Helper to convert normalized coordinate to screen pixel\n",
    "        def to_pixel(pt):\n",
    "            px = int(pt[0] * VISUAL_SCALE + OFFSET_X)\n",
    "            py = int(pt[1] * VISUAL_SCALE + OFFSET_Y)\n",
    "            return (px, py)\n",
    "\n",
    "        # 1. DRAW POSE\n",
    "        for start, end in POSE_CONNECTIONS:\n",
    "            p1_raw = pose[i][start]\n",
    "            p2_raw = pose[i][end]\n",
    "            \n",
    "            # Draw if not all zeros (masked pose check)\n",
    "            if not np.all(p1_raw == 0):\n",
    "                cv2.line(frame, to_pixel(p1_raw), to_pixel(p2_raw), (0, 255, 0), 3)\n",
    "\n",
    "        # 2. DRAW HANDS\n",
    "        # Anchors: LH Wrist is Pose 15, RH Wrist is Pose 16\n",
    "        lw_pose = pose[i][15]\n",
    "        rw_pos = pose[i][16]\n",
    "        \n",
    "        l_anchor = to_pixel(lw_pose) if not np.all(lw_pose == 0) else (WIDTH//3, HEIGHT//2)\n",
    "        r_anchor = to_pixel(rw_pos) if not np.all(rw_pos == 0) else (2*WIDTH//3, HEIGHT//2)\n",
    "        anchors = {'left': l_anchor, 'right': r_anchor}\n",
    "\n",
    "        # The \"Proportion Factor\" makes the hand 25% the size of the shoulder width\n",
    "        # This makes it look like a real human hand relative to the body\n",
    "        PROPORTION_FACTOR = 0.25 \n",
    "        \n",
    "        for side, hand_pts, color in [('left', lh[i], (255, 0, 0)), ('right', rh[i], (0, 0, 255))]:\n",
    "            if np.all(hand_pts == 0): continue\n",
    "            \n",
    "            # --- COMBINED SCALING ---\n",
    "            # 1. Divide by 5.0 to reverse training scaling\n",
    "            # 2. Multiply by PROPORTION_FACTOR to fit the body\n",
    "            current_hand = (hand_pts) / 5.0 * PROPORTION_FACTOR\n",
    "            \n",
    "            current_anchor = anchors[side]\n",
    "            \n",
    "            # Use a larger VISUAL_SCALE to see the person (e.g. 500-800)\n",
    "            # You can change VISUAL_SCALE at the top of draw_skeleton\n",
    "            \n",
    "            for start, end in HAND_CONNECTIONS:\n",
    "                p1 = (int(current_hand[start][0] * VISUAL_SCALE + current_anchor[0]), \n",
    "                      int(current_hand[start][1] * VISUAL_SCALE + current_anchor[1]))\n",
    "                p2 = (int(current_hand[end][0] * VISUAL_SCALE + current_anchor[0]), \n",
    "                      int(current_hand[end][1] * VISUAL_SCALE + current_anchor[1]))\n",
    "                cv2.line(frame, p1, p2, color, 2)\n",
    "\n",
    "            for pt in current_hand:\n",
    "                px = int(pt[0] * VISUAL_SCALE + current_anchor[0])\n",
    "                py = int(pt[1] * VISUAL_SCALE + current_anchor[1])\n",
    "                cv2.circle(frame, (px, py), 2, (255, 255, 255), -1)\n",
    "\n",
    "        # Add Frame Info and Text\n",
    "        cv2.putText(frame, f\"Frame: {i}\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 1)\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"âœ… Video saved: {output_path}\")\n",
    "\n",
    "# Run\n",
    "draw_generated_skeleton(FILE_PATH, OUTPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7d9f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¬ Processing 225 frames...\n",
      "âœ… Video created successfully: skeleton_check.mp4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "# --- SETTINGS ---\n",
    "FILE_PATH = \"../experiments/generated_output.npz\" # Path to your generated file\n",
    "OUTPUT_VIDEO = \"skeleton_check.mp4\"\n",
    "WIDTH, HEIGHT = 1000, 1000\n",
    "FPS = 60\n",
    "\n",
    "# Adjust these to fit your character perfectly on screen\n",
    "SKELETON_SCALE = 500  # Size of the body on screen\n",
    "HAND_VISUAL_SIZE = 3.5 # Relative size of the hands\n",
    "HAND_DATA_SCALE = 5.0  # MUST MATCH the scale used in your dataset.py\n",
    "\n",
    "# Colors (BGR)\n",
    "C_BODY = (0, 255, 0)     # Neon Green\n",
    "C_LH = (255, 255, 0)     # Cyan\n",
    "C_RH = (0, 0, 255)       # Red\n",
    "C_JOINTS = (255, 255, 255) # White\n",
    "\n",
    "# Connections Mapping\n",
    "POSE_CONN = [(11, 12), (11, 13), (13, 15), (12, 14), (14, 16), (11, 23), (12, 24), (23, 24)]\n",
    "HAND_CONN = [(0,1), (1,2), (2,3), (3,4), (0,5), (5,6), (6,7), (7,8), (9,10), (10,11), (11,12), \n",
    "             (13,14), (14,15), (15,16), (17,18), (18,19), (19,20), (0,17), (5,9), (9,13), (13,17)]\n",
    "\n",
    "def visualize_nsl(data_path, output_path):\n",
    "    # Load data\n",
    "    data = np.load(data_path)\n",
    "    pose, lh, rh = data['pose'], data['lh'], data['rh']\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, FPS, (WIDTH, HEIGHT))\n",
    "\n",
    "    print(f\"ðŸŽ¬ Processing {len(pose)} frames...\")\n",
    "\n",
    "    for i in range(len(pose)):\n",
    "        # Create professional dark background\n",
    "        frame = np.ones((HEIGHT, WIDTH, 3), dtype=np.uint8) * 15\n",
    "\n",
    "        # 1. Transform function: Projects (0,0) centered data to screen center\n",
    "        def to_screen(pt, offset_x=0, offset_y=0, custom_scale=SKELETON_SCALE):\n",
    "            x = int(pt[0] * custom_scale + WIDTH/2 + offset_x)\n",
    "            y = int(pt[1] * custom_scale + HEIGHT/2 + offset_y)\n",
    "            return (x, y)\n",
    "\n",
    "        # 2. Draw Pose\n",
    "        for start, end in POSE_CONN:\n",
    "            p1 = to_screen(pose[i][start])\n",
    "            p2 = to_screen(pose[i][end])\n",
    "            cv2.line(frame, p1, p2, C_BODY, 3, cv2.LINE_AA)\n",
    "\n",
    "        # 3. Draw Hands (Anchored to Pose Wrists)\n",
    "        # Landmark 15 = Left Wrist, 16 = Right Wrist\n",
    "        l_wrist_anchor = to_screen(pose[i][15])\n",
    "        r_wrist_anchor = to_screen(pose[i][16])\n",
    "        \n",
    "        anchors = {'left': l_wrist_anchor, 'right': r_wrist_anchor}\n",
    "        hands_data = {'left': lh[i], 'right': rh[i]}\n",
    "        colors = {'left': C_LH, 'right': C_RH}\n",
    "\n",
    "        for side in ['left', 'right']:\n",
    "            h_pts = hands_data[side]\n",
    "            if np.all(h_pts == 0): continue\n",
    "            \n",
    "            # UN-SCALE: Reverse the 5x scaling from training\n",
    "            h_pts = h_pts / HAND_DATA_SCALE \n",
    "            \n",
    "            anchor = anchors[side]\n",
    "            \n",
    "            # Draw bones\n",
    "            for s, e in HAND_CONN:\n",
    "                # Project fingers relative to the wrist anchor\n",
    "                p1 = (int(h_pts[s][0] * SKELETON_SCALE * HAND_VISUAL_SIZE + anchor[0]),\n",
    "                      int(h_pts[s][1] * SKELETON_SCALE * HAND_VISUAL_SIZE + anchor[1]))\n",
    "                p2 = (int(h_pts[e][0] * SKELETON_SCALE * HAND_VISUAL_SIZE + anchor[0]),\n",
    "                      int(h_pts[e][1] * SKELETON_SCALE * HAND_VISUAL_SIZE + anchor[1]))\n",
    "                cv2.line(frame, p1, p2, colors[side], 2, cv2.LINE_AA)\n",
    "\n",
    "            # Draw joint dots\n",
    "            for pt in h_pts:\n",
    "                px = int(pt[0] * SKELETON_SCALE * HAND_VISUAL_SIZE + anchor[0])\n",
    "                py = int(pt[1] * SKELETON_SCALE * HAND_VISUAL_SIZE + anchor[1])\n",
    "                cv2.circle(frame, (px, py), 3, C_JOINTS, -1, cv2.LINE_AA)\n",
    "\n",
    "        # 4. Info Overlay\n",
    "        cv2.putText(frame, f\"FRAME: {i}\", (30, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 100, 100), 2)\n",
    "        cv2.putText(frame, \"NSL FINGERSPELLING GENERATOR\", (30, HEIGHT-30), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (70, 70, 70), 1)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"âœ… Video created successfully: {output_path}\")\n",
    "\n",
    "# Execute\n",
    "visualize_nsl(FILE_PATH, OUTPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91336ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "FILE_PATH = \"../experiments/generated_output.npz\"\n",
    "\n",
    "# FILE_PATH = \"../training_dataset/sequences/consonant/S1_NSL_Consonant_Bright/D_SHA.npz\"\n",
    "OUTPUT_VIDEO = \"skeleton_check.mp4\"\n",
    "WIDTH, HEIGHT = 1000, 1000\n",
    "FPS = 60\n",
    "\n",
    "# MediaPipe Hand Connections\n",
    "HAND_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "    (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "    (5, 9), (9, 10), (10, 11), (11, 12),\n",
    "    (9, 13), (13, 14), (14, 15), (15, 16),\n",
    "    (13, 17), (0, 17), (17, 18), (18, 19), (19, 20)\n",
    "]\n",
    "\n",
    "def draw_generated_hands_only(data_path, output_path):\n",
    "    data = np.load(data_path)\n",
    "    lh = data['lh']  # (Frames, 21, 3)\n",
    "    rh = data['rh']  # (Frames, 21, 3)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, FPS, (WIDTH, HEIGHT))\n",
    "\n",
    "    VISUAL_SCALE = 800\n",
    "\n",
    "    # Fixed anchors so hands don't depend on pose\n",
    "    LEFT_ANCHOR  = (WIDTH // 3, HEIGHT // 2)\n",
    "    RIGHT_ANCHOR = (2 * WIDTH // 3, HEIGHT // 2)\n",
    "\n",
    "    print(f\"Generating hand-only video for {len(lh)} frames...\")\n",
    "\n",
    "    for i in range(len(lh)):\n",
    "        frame = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "        for hand_pts, anchor, color in [\n",
    "            (lh[i], LEFT_ANCHOR, (255, 0, 0)),\n",
    "            (rh[i], RIGHT_ANCHOR, (0, 0, 255))\n",
    "        ]:\n",
    "            if np.all(hand_pts == 0):\n",
    "                continue\n",
    "\n",
    "            # Reverse training scale and normalize size\n",
    "            hand = hand_pts / 5.0\n",
    "\n",
    "            # Draw bones\n",
    "            for s, e in HAND_CONNECTIONS:\n",
    "                p1 = (\n",
    "                    int(hand[s][0] * VISUAL_SCALE + anchor[0]),\n",
    "                    int(hand[s][1] * VISUAL_SCALE + anchor[1])\n",
    "                )\n",
    "                p2 = (\n",
    "                    int(hand[e][0] * VISUAL_SCALE + anchor[0]),\n",
    "                    int(hand[e][1] * VISUAL_SCALE + anchor[1])\n",
    "                )\n",
    "                cv2.line(frame, p1, p2, color, 2)\n",
    "\n",
    "            # Draw joints\n",
    "            for pt in hand:\n",
    "                px = int(pt[0] * VISUAL_SCALE + anchor[0])\n",
    "                py = int(pt[1] * VISUAL_SCALE + anchor[1])\n",
    "                cv2.circle(frame, (px, py), 3, (255, 255, 255), -1)\n",
    "\n",
    "        cv2.putText(frame, f\"Frame: {i}\", (30, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 1)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"âœ… Hand-only video saved: {output_path}\")\n",
    "\n",
    "# Run\n",
    "draw_generated_hands_only(FILE_PATH, OUTPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9192c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating hand-only video for 200 frames...\n",
      "âœ… Hand-only video saved: skeleton_check.mp4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# experiments\\eval_samples\\epoch_50_sample_A.npz\n",
    "FILE_PATH = \"../experiments/generated_output.npz\"\n",
    "\n",
    "# FILE_PATH = \"../training_dataset/sequences/NSL_Consonant_Multi/S3_NSL_Consonant_Prepared/S3_all_consonant_Phone_Camera/JHA_989_1064.npz\"\n",
    "OUTPUT_VIDEO = \"skeleton_check.mp4\"\n",
    "WIDTH, HEIGHT = 1000, 1000\n",
    "FPS = 60\n",
    "\n",
    "# MediaPipe Hand Connections\n",
    "HAND_CONNECTIONS = [\n",
    "    (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "    (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "    (5, 9), (9, 10), (10, 11), (11, 12),\n",
    "    (9, 13), (13, 14), (14, 15), (15, 16),\n",
    "    (13, 17), (0, 17), (17, 18), (18, 19), (19, 20)\n",
    "]\n",
    "\n",
    "def draw_generated_hands_only(data_path, output_path):\n",
    "    data = np.load(data_path)\n",
    "    lh = data['lh']  # (Frames, 21, 3)\n",
    "    rh = data['rh']  # (Frames, 21, 3)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, FPS, (WIDTH, HEIGHT))\n",
    "\n",
    "    VISUAL_SCALE = 300\n",
    "\n",
    "    # Fixed anchors so hands don't depend on pose\n",
    "    LEFT_ANCHOR  = (WIDTH // 3, HEIGHT // 2)\n",
    "    RIGHT_ANCHOR = (2 * WIDTH // 3, HEIGHT // 2)\n",
    "\n",
    "    print(f\"Generating hand-only video for {len(lh)} frames...\")\n",
    "\n",
    "    for i in range(len(lh)):\n",
    "        frame = np.zeros((HEIGHT, WIDTH, 3), dtype=np.uint8)\n",
    "\n",
    "        for hand_pts, anchor, color in [\n",
    "            (lh[i], LEFT_ANCHOR, (255, 0, 0)),\n",
    "            (rh[i], RIGHT_ANCHOR, (0, 0, 255))\n",
    "        ]:\n",
    "            if np.all(hand_pts == 0):\n",
    "                continue\n",
    "\n",
    "            hand = hand_pts \n",
    "\n",
    "            for s, e in HAND_CONNECTIONS:\n",
    "                p1 = (\n",
    "                    int(hand[s][0] * VISUAL_SCALE + anchor[0]),\n",
    "                    int(hand[s][1] * VISUAL_SCALE + anchor[1])\n",
    "                )\n",
    "                p2 = (\n",
    "                    int(hand[e][0] * VISUAL_SCALE + anchor[0]),\n",
    "                    int(hand[e][1] * VISUAL_SCALE + anchor[1])\n",
    "                )\n",
    "                cv2.line(frame, p1, p2, color, 2)\n",
    "\n",
    "            for pt in hand:\n",
    "                px = int(pt[0] * VISUAL_SCALE + anchor[0])\n",
    "                py = int(pt[1] * VISUAL_SCALE + anchor[1])\n",
    "                cv2.circle(frame, (px, py), 3, (255, 255, 255), -1)\n",
    "\n",
    "        cv2.putText(frame, f\"Frame: {i}\", (30, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 1)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"âœ… Hand-only video saved: {output_path}\")\n",
    "\n",
    "# Run\n",
    "draw_generated_hands_only(FILE_PATH, OUTPUT_VIDEO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471f0a8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FYP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
